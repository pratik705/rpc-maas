---
ceph_cluster_name: ceph

# Error metric info for detailed ceph health checks.
# Descriptions pulled from:  https://docs.ceph.com/docs/master/rados/operations/health-checks
ceph_health_mon_checks:
  - name: 'MON_DOWN'
    description: 'One or more monitor daemons is currently down. The cluster requires a majority (more than 1/2) of the monitors in order to function'
  - name: 'MON_CLOCK_SKEW'
    description: 'The clocks on the hosts running the ceph-mon monitor daemons are not sufficiently well synchronized.'
  - name: 'MON_MSGR2_NOT_ENABLED'
    description: 'The ms_bind_msgr2 option is enabled but one or more monitors is not configured to bind to a v2 port in the clusterâ€™s monmap. '
  - name: 'MON_DISK_LOW'
    description: 'One or more monitors is low on disk space(default below 30%).'
  - name: 'MON_DISK_CRIT'
    description: 'One or more monitors is low on disk space(default below 5%).'
  - name: 'MON_DISK_BIG'
    description: 'Monitor database is larger than mon_data_size_warn'

ceph_health_mgr_checks:
  - name: 'MGR_DOWN'
    description: 'All manager daemons are currently down. The cluster should normally have at least one running manager (ceph-mgr) daemon.'
  - name: 'MGR_MODULE_DEPENDENCY'
    description: 'An enabled manager module is failing its dependency check.'
  - name: 'MGR_MODULE_ERROR'
    description: 'A manager module has experienced an unexpected error.'

ceph_health_osds_checks:
  - name: 'OSD_DOWN'
    description: 'One or more OSDs are marked down.'
  - name: 'OSD_HOST_DOWN'
    description: 'All the OSDs within a particular CRUSH subtree are marked down, for example all OSDs on a host.'
  - name: 'OSD_ROOT_DOWN'
    description: 'All the OSDs within a particular CRUSH subtree are marked down, for example all OSDs on a host.'
  - name: 'OSD_ORPHAN'
    description: 'An OSD is referenced in the CRUSH map hierarchy but does not exist.'
  - name: 'OSD_OUT_OF_ORDER_FULL'
    description: 'The utilization thresholds for nearfull, backfillfull, full, and/or failsafe_full are not ascending.'
  - name: 'OSD_FULL'
    description: 'One or more OSDs has exceeded the full threshold and is preventing the cluster from servicing writes.'
  - name: 'OSD_BACKFILLFULL'
    description: 'One or more OSDs has exceeded the backfillfull threshold, which will prevent data from being allowed to rebalance to this device.'
  - name: 'OSD_NEARFULL'
    description: 'One or more OSDs has exceeded the nearfull threshold. This is an early warning that the cluster is approaching full.'
# Disabled per ceph team request
#  - name: 'OSDMAP_FLAGS'
#    description: 'One or more cluster flags of interest has been set.(full,pauserd,pausewr,noup,nodown,noin,noout,nobackfill,noscrub,nodeep_scrub or notieragent)'
  - name: 'OSD_FLAGS'
    description: 'One or more OSDs or CRUSH (nodes,device classes) has a flag of interest set.(noup,nodown,noin,noout)'
  - name: 'OLD_CRUSH_TUNABLES'
    description: 'The CRUSH map is using very old settings and should be updated.'
  - name: 'OLD_CRUSH_STRAW_CALC_VERSION'
    description: 'The CRUSH map is using an older, non-optimal method for calculating intermediate weight values for straw buckets.'
  - name: 'CACHE_POOL_NO_HIT_SET'
    description: 'One or more cache pools is not configured with a hit set to track utilization, which will prevent the tiering agent from identifying cold objects to flush and evict from the cache.'
  - name: 'OSD_NO_SORTBITWISE'
    description: 'No pre-luminous v12.y.z OSDs are running but the sortbitwise flag has not been set.'
  - name: 'BLUEFS_SPILLOVER'
    description: 'One or more OSDs that use the BlueStore backend have been allocated db partitions but that space has filled causing a spillover onto the normal slow device.'
  - name: 'BLUEFS_AVAILABLE_SPACE'
    description: 'Bluefs available space issues.'
  - name: 'BLUEFS_LOW_SPACE'
    description: 'BlueFS is running low on available free space.'
  - name: 'BLUESTORE_FRAGMENTATION'
    description: 'As BlueStore works free space on underlying storage will get fragmented.'
  - name: 'BLUESTORE_LEGACY_STATFS'
    description: 'One or more OSDs have BlueStore volumes that were created prior to Nautilus'
  - name: 'BLUESTORE_NO_PER_POOL_OMAP'
    description: 'One or more OSDs have volumes that were created prior to Octopus'
  - name: 'BLUESTORE_DISK_SIZE_MISMATCH'
    description: 'One or more OSDs using BlueStore has an internal inconsistency between the size of the physical device and the metadata tracking its size.'
  - name: 'BLUESTORE_NO_COMPRESSION'
    description: 'One or more OSDs is unable to load a BlueStore compression plugin.'
  - name: 'BLUESTORE_SPURIOUS_READ_ERRORS'
    description: 'One or more OSDs using BlueStore detects spurious read errors at main device.'


ceph_health_device_health_checks:
  - name: 'DEVICE_HEALTH'
    description: 'One or more devices is expected to fail soon, where the warning threshold is controlled by the mgr/devicehealth/warn_threshold config option.'
  - name: 'DEVICE_HEALTH_IN_USE'
    description: 'One or more devices is expected to fail soon and has been marked out of the cluster based on mgr/devicehealth/mark_out_threshold, but it is still participating in one more PGs.'
  - name: 'DEVICE_HEALTH_TOOMANY'
    description: 'Too many devices is expected to fail soon and the mgr/devicehealth/self_heal behavior is enabled, such that marking out all of the ailing devices would exceed the clusters mon_osd_min_in_ratio ratio that prevents too many OSDs from being automatically marked out.'

ceph_health_data_health_checks:
  - name: 'PG_AVAILABILITY'
    description: 'One or more PGs is in a state that does not allow IO requests to be serviced.'
  - name: 'PG_DEGRADED'
    description: 'One or more PGs has the degraded or undersized flag set or has not had the clean flag set for some time.'
  - name: 'PG_RECOVERY_FULL'
    description: 'One or more PGs has the recovery_toofull flag set. One or more OSDs is above the full threshold.'
  - name: 'PG_BACKFILL_FULL'
    description: 'Specifically, one or more PGs has the backfill_toofull flag set. One ore more OSDs is above the backfillfull threshold.'
  - name: 'PG_DAMAGED'
    description: 'Data scrubbing has discovered some problems with data consistency in the cluster.'
  - name: 'OSD_SCRUB_ERRORS'
    description: 'Recent OSD scrubs have uncovered inconsistencies. This error is generally paired with PG_DAMAGED.'
  - name: 'OSD_TOO_MANY_REPAIRS'
    description: 'Possible failing disk as repairs have exceeded mon_osd_warn_num_repaired(default 10)'
  - name: 'LARGE_OMAP_OBJECTS'
    description: 'One or more pools contain large omap objects as determined by osd_deep_scrub_large_omap_object_key_threshold, osd_deep_scrub_large_omap_object_value_sum_threshold or both.'
  - name: 'CACHE_POOL_NEAR_FULL'
    description: 'A cache tier pool is nearly full.'
  - name: 'TOO_FEW_PGS'
    description: 'The number of PGs in use in the cluster is below the configurable threshold of mon_pg_warn_min_per_osd PGs per OSD.'
  - name: 'POOL_PG_NUM_NOT_POWER_OF_TWO'
    description: 'One or more pools has a pg_num value that is not a power of two.'
  - name: 'POOL_TOO_FEW_PGS'
    description: 'One or more pools should probably have more PGs, based on the amount of data that is currently stored in the pool.'
  - name: 'TOO_MANY_PGS'
    description: 'The number of PGs in use in the cluster is above the configurable threshold of mon_max_pg_per_osd PGs per OSD'
  - name: 'POOL_TOO_MANY_PGS'
    description: 'One or more pools should probably have less PGs, based on the amount of data that is currently stored in the pool.'
  - name: 'POOL_TARGET_SIZE_BYTES_OVERCOMMITTED'
    description: 'One or more pools have a target_size_bytes property set to estimate the expected size of the pool, but the value(s) exceed the total available storage.'
  - name: 'POOL_HAS_TARGET_SIZE_BYTES_AND_RATIO'
    description: 'One or more pools have both target_size_bytes and target_size_ratio set to estimate the expected size of the pool.'
  - name: 'TOO_FEW_OSDS'
    description: 'The number of OSDs in the cluster is below the configurable threshold of osd_pool_default_size.'
  - name: 'SMALLER_PGP_NUM'
    description: 'One or more pools has a pgp_num value less than pg_num.'
  - name: 'MANY_OBJECTS_PER_PG'
    description: 'One or more pools has an average number of objects per PG that is significantly higher than the overall cluster average.'
  - name: 'POOL_APP_NOT_ENABLED'
    description: 'A pool exists that contains one or more objects but has not been tagged for use by a particular application.'
  - name: 'POOL_FULL'
    description: 'One or more pools has reached (or is very close to reaching) its quota.'
  - name: 'POOL_NEAR_FULL'
    description: 'One or more pools is approaching is quota.'
# Disabled per ceph team request
#  - name: 'OBJECT_MISPLACED'
#    description: 'One or more objects in the cluster is not stored on the node the cluster would like it to be stored on.(Possible incomplete migration)'
  - name: 'OBJECT_UNFOUND'
    description: 'One or more objects in the cluster cannot be found.'
  - name: 'SLOW_OPS'
    description: 'One or more OSD requests is taking a long time to process. This can be an indication of extreme load, a slow storage device, or a software bug.'
  - name: 'PG_NOT_SCRUBBED'
    description: 'One or more PGs has not been scrubbed recently.'
  - name: 'PG_NOT_DEEP_SCRUBBED'
    description: 'One or more PGs has not been deep scrubbed recently.'
  - name: 'PG_SLOW_SNAP_TRIMMING'
    description: 'The snapshot trim queue for one or more PGs has exceeded the configured warning threshold.'

ceph_health_misc_checks:
  - name: 'RECENT_CRASH'
    description: 'One or more Ceph daemons has crashed recently, and the crash has not yet been archived (acknowledged) by the administrator.'
  - name: 'TELEMETRY_CHANGED'
    description: 'Telemetry has been enabled, but the contents of the telemetry report have changed since that time, so telemetry reports will not be sent.'
  - name: 'AUTH_BAD_CAPS'
    description: 'One or more auth users has capabilities that cannot be parsed by the monitor.'
  - name: 'OSD_NO_DOWN_OUT_INTERVAL'
    description: 'The mon_osd_down_out_interval option is set to zero, which means that the system will not automatically perform any repair or healing operations after an OSD fails.'


